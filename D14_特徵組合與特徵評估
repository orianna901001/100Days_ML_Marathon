特徵組合介紹與緣由：透過某種運算方式，將有關聯性的特徵合併在一起，成為新的特徵，減少關聯性高的特徵所造成的干擾，也能增加資訊量。
  1. 連續變數與連續變數之間的組合 eg：身高與體重
  2. 類別變數與類別變數之間的組合 eg：性別與是否有心臟病兩個類別變數。
  3. 連續變數與類別變數之間的組合 eg：當某個連續變數在某個二分類類別變數中不同類別會有顯著差異，代表該類別變數的不同類別會影響連續變數的數值，因此需要將該兩變數進行「交互作用」，為兩數相乘。

特徵評估介紹與緣由：
  關鍵問題是當產生新的特徵後，如何確定新特徵能夠有效影響目標？
    1. 挑選特徵過程中，其實就間接知道該特徵對模型的幫助為何，當某特徵沒有被挑選到，代表它有很大機會是會對模型產生負面效果；反之，代表該特徵是對模型有很好的效果。
    2. 模型建立後，為確認每個特徵對模型預測力的重要程度，開發出數種不同重要性指標的計算方法。
    3. 例如：在使用三種特徵重要性方法後，A 特徵皆第一重要，B 特徵則分別為第一重要與第二重要，兩者相較之下，A 特徵的重要程度比 B 特徵高。

樹型演算法（包含決策樹、隨機森林、XGBoost、LightGBM、CatBoost 等）為基礎的重要性：
  Step1. 以某個特徵作為基準，訂定區分條件後，將所有資料依照條件分成兩個分支。（例如：假設以「房間坪樹」作為基準，則為「<= 25」的資料會被分到左邊的分支、「> 25」的資料會被分到右邊的分支。）
  Step2. 計算建立該分支的 Entropy 或 Gini。
  Step3. 重複 Step1 與 Step2 直到所有特徵都求得 Entropy 或 Gini
  Step4. 挑出最大 Entropy 或 Gini 的特徵，代表第一次分支會使用到該特徵。
  Step5. 重複上述步驟，不斷往下產生分支，直到模型建立完成。
->
    1. 越早被挑到當作基準的特徵越重要。
    2. 被挑到越多次當作基準的特徵越重要。
    3. Entropy 或 Gini 總和數值越高的越重要。
    4. 特徵覆蓋度與 Entropy 或 Gin 綜合計算求得的重要性。

Permutation Importance：是最常見且泛用的特徵重要性演算法，不限於樹型演算法，神經網路、KNN 或線性迴歸也都可以使用。
  關鍵步驟：透過調換某個特徵的數值，以確認最終結果是否會被影響。
    
  
