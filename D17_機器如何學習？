線性迴歸模型（linear regression）：𝑥𝑖代表的是第𝑖筆的輸入或是特徵，yi則是第𝑖筆的輸出或是標籤，可以輸入特徵𝑥 讓模型𝑓 預測標籤 𝑦，函式f(x) = w x + b 中模型的參數w及b則是未知的。
所以函式這條線最好可以去貼合資料，這樣的過程我們稱為擬合（fitting）

誤差函數，我們稱它為均方誤差（mean square error），它可以計算平均每個資料點到趨勢線的垂直距離
損失函數（loss function）是用來描述一個模型跟資料之間的不貼合程度的函數
最小化損失函數：讓w跟b最小化，我們可以把模型參數 𝑚跟 b 分別畫到 x 跟 𝑦 軸，並且把對應的損失函數 𝐿(𝑦,𝑦^) 的值畫到 𝑧軸就會形成一個拋物面。
使用梯度下降法找到拋物面中最小的點
 第一個是模型本身，模型本身應該要符合資料的趨勢，第二是損失函數或是誤差函數，損失函數應該要根據要預測的任務去設計，不同的任務有不同的損失函數，最後是學習演算法，學習演算法是要去學習到模型的參數的。
 如果把這些元素加以抽換，或是排列組合，就可以變成不同的機器學習模型。例如，把線性迴歸模型的模型加上 sigmoid function，並且把損失函數換成交叉熵（cross entropy），它就變成了羅吉斯迴歸模型。
 
 
 
 
 
 
 
 
 
 

