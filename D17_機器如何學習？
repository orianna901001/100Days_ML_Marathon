線性迴歸模型（linear regression）：𝑥𝑖代表的是第𝑖筆的輸入或是特徵，yi則是第𝑖筆的輸出或是標籤，可以輸入特徵𝑥 讓模型𝑓 預測標籤 𝑦，函式f(x) = w x + b 中模型的參數w及b則是未知的。
所以函式這條線最好可以去貼合資料，這樣的過程我們稱為擬合（fitting）

誤差函數，我們稱它為均方誤差（mean square error），它可以計算平均每個資料點到趨勢線的垂直距離
\frac{1}{n} \sum_{I=1}^n (y_i - \hat{y}<em>i)^2 

損失函數（loss function）是用來描述一個模型跟資料之間的不貼合程度的函數
\mathcal{L}(y, \hat{y}) = \frac{1}{n} \sum_{I=1}^n (y_i - \hat{y}<em>i)^2

最小化損失函數：
\mathop{\arg\min}<em>{w,b} \mathcal{L}(y, \hat{y}) = \mathop{\arg\min}</em>{w,b} \frac{1}{n} \sum_{I=1}^n (y_i - \hat{y}<em>i)^2 = \mathop{\arg\min}</em>{w,b} \frac{1}{n} \sum_{I=1}^n (y_i - (w x_i + b))^2
讓w跟b最小化，我們可以把模型參數 𝑚跟 b 分別畫到 x 跟 𝑦 軸，並且把對應的損失函數 𝐿(𝑦,𝑦^) 的值畫到 𝑧軸就會形成一個拋物面。

為了找到拋物面中最小的點，使用梯度下降法：
\theta_{t+1} = \theta_{t} - \eta \nabla \mathcal{L}(y, \hat{y}; \theta_{t})
\eta代表的是步伐的大小。
 
 
 第一個是模型本身，模型本身應該要符合資料的趨勢，第二是損失函數或是誤差函數，損失函數應該要根據要預測的任務去設計，不同的任務有不同的損失函數，最後是學習演算法，學習演算法是要去學習到模型的參數的。
 如果把這些元素加以抽換，或是排列組合，就可以變成不同的機器學習模型。例如，把線性迴歸模型的模型加上 sigmoid function，並且把損失函數換成交叉熵（cross entropy），它就變成了羅吉斯迴歸模型。
 
 
 
 
 
 
 
 
 
 

