整個機器學習透過損失函數，可以評估這個模型對於資料的適合度（fitness）有多高，為了去提高適合度，利用了梯度下降法去調整模型參數，並降低損失函數的值。
可以發現透過損失函數，將一個模型的學習問題轉化成了一個最佳化問題。

f(x)通常稱為目標函數（target function），或稱為代價函數（cost function），是要藉由去調整變數x使f(x)最大化或最小化，目標函數是要用在最大化arg max的情況，而代價函數或損失函式(loss function)則是用在最小化arg min的情況。
在最佳化問題中，有可能會對變數x有所限制，設定a為常數為沒有限制式的最佳化問題，稱為無約束最佳化（unconstrained optimization）；帶有限制式的則稱為約束最佳化（constrained optimization）。

在機器學習中常遇到的問題是局部最佳解問題，我們想要求的是全局最佳解，因此歷史上有很多解法：
1. 基因演算法（genetic algorithm）
2. 模擬退火法（simulated annealing）
3. 粒子群演算法（particle swarm algorithm）
4. 蟻群演算法 (ant colony optimization)
5. 蜂群演算法 (bee colony algorithm)

鞍點（saddle point）：鞍點的區域附近會造成梯度的計算上會趨近於 0，，求到的解不是局部最佳解，也不是全局最佳解，可以使用帶有動量（momentum）的梯度方法。
多目標最佳化問題：眾多的條件希望可以同時達到最佳，在線上的點都是最佳解，只是有不同的分配比例。
凸函數最佳化：他的形狀是「凸」向下的，沒有其他的山丘之類的形狀。
非凸函數最佳化問題：非凸函數最佳化問題（nonconvex optimization）仍然是一個學術上待解的問題，目前已經有非常多的方法提出，不過仍然缺乏一個系統性的解決方法。


基於梯度的方法（gradient-based methods）
  梯度下降法（Gradient descent method）
  共軛梯度法（Conjugate gradient method）
  Coordinate descent methods
  ...
基於 Hessians 的方法
  Newton's method
  Interior point methods
  隨機方法（stochastic methods）
馬可夫蒙地卡羅法（Markov chain Monte Carlo）
  Metropolis-Hastings method
  Hamiltonian Monte Carlo
  Variational inference
  Gibb's sampling
  ...
連續空間的方法（methods for continuous space）
  變分法（Calculus of variations）
  偏微分方程（Partial differential equations）



1. 對所有的v而言，v的轉置矩陣*H*v > 0 
			→H為positive definite矩陣，其特性為eigenvalue都是正數
			→只要觀察H的eigenvalue是不是全都正數，便可知道v的轉置矩陣*H*v是否>0
			→Local minima
	2. 對所有的v而言，v的轉置矩陣*H*v < 0 
			→H為negative definite矩陣，其特性為eigenvalue都是負數
			→只要觀察H的eigenvalue是不是全都負數，便可知道v的轉置矩陣*H*v是否<0
			→Local maxima
	3. 對所有的v而言，v的轉置矩陣*H*v有時>0有時<0
			→只要觀察H的eigenvalue是不是有正有負
			→Saddle point
結論
	只要算出Hessian這個矩陣，並且觀察其eigenvalue，便可知道現在的Critical point是什麼情況


	把LF寫出來，L=(y_hat-w1w2x)^2=(1-w1w2)^2
	現在來求，若w1w2都是0，此點是什麼情況？
	求Gradient
	Gradient有兩個式子，分別是
		1.w1對L的偏微分
		2.w2對L的偏微分
	然後求Hessian，為一個2維矩陣，有四個式子，分別是
		1.w1對L微兩次
		2.w2微w1微
		3.w1微w2微
		4.w2微兩次
	把(0,0)帶入Hessian，可得4個式子答案分別是0，-2，-2，0
	再求eigenvalue，可發現其為2與-2
	因此此點為saddle point
