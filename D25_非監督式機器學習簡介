非監督式學習：基於輸入資料找出模式，當我們無法確定尋找內容，或無標記 (y) 資料時，通常會用這個演算法，幫助我們了解資料模式、資料特性，或作為提升監督式學習效能的預處理步驟。
非監督式學習由於沒有監督式學習算法所參照的 ground truth，不易判斷模型是否真的學到隱藏資料中的模式，需要透過像是「輪廓分析」等方法評估分群的品質。

(1)  分群（Clustering）也稱為聚類分析、集群分析，將擁有各自相似屬性的樣本聚到各自一群，使其成為不同的群體。分群結果會給予每個樣本點一個標籤（label）。
    通常分群後，會計算各群特徵的統計指標（如圖 54-2 所示），方便我們比較不同群間的差異，也可當作分群解釋方法。
    建議群數不超過資料集的特徵維度數
(2)  關聯規則（Association Rule）從大量數據中發現變數間隱藏關係的方法
(3)  異常檢測（Anomaly Detection）透過樣本特徵的群聚，將相對異常的模式、樣本或事件辨識出來（如圖 54-4 所示），常見應用如：交易詐欺、結構缺陷檢測、醫療問題、文字錯誤辨識、入侵檢測等。
(4)  降維（Dimension Reduction）當特徵數太多難於理解及呈現的情況下，藉由抽象化的技術轉換原資料的表示方式、降低資料維度、組合成新的特徵；同時不失去原有的資訊。
(5)  非結構化資料分析非結構化資料如文字、影像等，可以藉由非監督式學習技術，幫助呈現、描述並在一系列資料中萃取關鍵字，如主題模型（topic model）（如圖 54-6），屬於一種非監督式的分類方法。

非監督學習的演算法分類與概要：
(1)分群法：
(1.1)聚合式階層分群法（Agglomerative Hierarchical Clustering）：將每個樣本視為一個群聚，從樹狀結構底部不斷融合相近的樣本；假如生成的群數多於我們預期的群數，則反覆重複聚合最近距離的兩群的動作，直到群數降到條件範圍內。
(1.2)分割式分群（Partitional Clustering）：以 K-Means 演算法 為例，把 n 個點切分到 k 個群中，不斷重複調整群心、重新分群，最後達到每個點都隸屬於與其群心最近的聚類（群）中。
(2)關聯規則 
(2.1)Apriori 演算法：一種逐層搜索的迭代方法。先找出出現頻次為 1 的項目集合，存為 L1；再用 L1 找出現頻次為 2 的項目集合 L2，L2 再用來找 L3，依此類推，直到不能找到更多頻次共同出現的項目集合。
(3)異常檢測 
(3.1)LOF（Local Outlier Factor）：如果樣本點 p 的 LOF 得分接近 1，則表示 p 的局部密度與相鄰點差不多 如果 p 的 LOF 得分小於 1，則表示 p 落在一個相對密集的區域，最不像是異常點 如果 p 的 LOF 得分遠大於 1，則表示 p 跟其他樣本點相對疏遠，很有可能是異常點
(4)降維
(4.1)主成分分析（Principle Component Analysis, PCA）：對一系列可能相關的特徵進行線性轉換，將他們投影成另一系列「線性不相關」的特徵值，這些不相關特徵稱為「主成分」
(5)結構化資料分析 
(5.1)隱含狄利克雷分布（Latent Dirichlet Allocation, LDA）：事先定義好有限的主題，並透過觀察文件與朋友用詞來計算出主題之間的關聯，以及各個文件的主題分佈，如此一來，只要文件夠多，就可以有效的快速理解不同文件的主題分佈



