特徵挑選：  
    透過不同方法，找到對預測模型來說很重要的特徵。
    可透過領域知識、統計方法或是機器學習演算法找到有用的特徵。
    不只可挑出重要特徵，也間接達到「降維」的效果。

特徵挑選方法主要可分成三大類：過濾法（Filters）、（Wrappers）、（Hybrid）

Exhaustive Feature Selection：
  找到所有特徵組合可能性，逐一配適模型後，找到最佳的特徵組合。
  是個最耗時間，但答案最肯定的方法。

Sequential Forward或Backward Feature Selection會分別從沒有特徵與所有特徵，透過適合的評估指標決定哪個特徵需要放入或排除。
SFS：每次從剩餘特徵中，加上最能提升評分的特徵。
BFS：每次從目前特徵中，移除對評分影響最小的特徵。

Sequential Floating Forward或Backward Feature Selection則是分別持續放入與排除特徵過程中，分別穿插排除與放入特徵，以達到配對更多不同組合的目的。
SFFS：
  前向擴充：從空集開始，每一步先依評估指標（如交叉驗證誤差、AIC、F-score 等）貪婪地挑選一個「最佳新增」特徵加入。
  條件後退：在每次新增之後，檢查已選特徵集中是否有任何一個特徵，若將其 移除 可進一步提升模型評分，則執行移除。
SFBS：
  後向刪除：從全特徵集開始，每一步先依評估指標貪婪地挑選一個「最不重要」特徵刪除。
  條件前進：在每次刪除之後，檢查已刪特徵集中是否有任何一個先前被移除的特徵，若將其 重新加入 可進一步提升模型評分，則執行加入。

RFE或RFECV
RFE ：每一輪只會建立一個模型，也只能用該模型進行特徵重要性的計算
RFECV ：每一輪會由 K-fold Cross-Validation 產生的資料建立出 K 個模型，並會以預測力最佳的模型作為最終模型，加以計算該模型的特徵重要性
