數值特徵：
    找到好的特徵以反應最終預測目標。
    將不同數值特徵範圍與尺度單位統一化。
    用「訓練資料」定義特徵工程的規則或函數，並套用在「驗證資料」或「測試資料」中。

特徵縮放(Feature Scaling)：特徵縮放會將所有特徵限縮在同一範圍內。
  1. Max-Abs Scale 將特徵的數值範圍轉換成 [-1, 1] 之間
  2. Min-Max Scale 將特徵的數值範圍轉換成 [0, 1] 之間。

特徵標準化(Feature Normalization)特徵標準化會將每個特徵的數值分布轉換成統一數值分布。
  1. Z 分數標準化(Z-Score Normalization) 將數值轉換成標準常態分佈（平均數為 0、標準差為 1）的狀態。
  2. Quantile Normalization ：將數值轉換成範圍從 0 至 1 的連續均勻分布

特徵轉換(Feature Transformation)
  1. 對數轉換(Log Transformation)：將小於 1 的數值轉換成負數，並將大於 1 的數值縮小，使得特徵整體範圍接近 0。
  2. 根號轉換(Root Transformation)：將小於 1 的數值放大，並將大於 1 的數值縮小，使得特徵整體範圍接近 1
  3. 指數轉換(Exponential Transformation)：數值較小者會放大，但數值大者會被放得更大，具有強調的效果。

特徵區間化(Feature Binning)
  1. 數值範圍
  2. 類別頻率
  3. 專業知識

數值特徵之遺失值：
  1. 隨機遺失(Random Missingness)該資料的遺失屬於非特定因素、無特別有意義的原因，導致遺失值隨機產生在資料中 eg：人為
  2. 非隨機遺失(Non-Random Missingness)：遺失資料的產生是特別具有意義的，例如：當病患未患有高血壓，就不會有服用高血壓藥物的種類。因每個遺失資料都具有意義的資訊，需特過許多條件與資料前處理清楚定義遺失值的意義，諸如給予一個類別並表明遺失值的原因等。


  統計數值填補 eg：平均數、中位數。  
    劣勢：忽視特徵之間的複雜關係、改變數值特徵的機率分布。
  迴歸模型填補 (Regression Imputation)：
    以具遺失值的特徵作為預測目標，無遺失值資料的特徵作為特徵，建立迴歸模型，最後再預測出有遺失的數值。
    模型的選擇將影響後續預測的效果，導致特徵工程複雜性增加。
  K 近鄰演算法(K-Nearest Neighbor; KNN) 填補：與迴歸模型填補法相似
  以模型為導向的填補技巧(Model-Driven Imputation：
    示範三種填補技巧的比較，可發現使用隨機森林的填補技巧，對後續的分類任務而言非常有效果。
