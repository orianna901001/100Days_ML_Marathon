監督式學習主要分為迴歸問題與分類問題。

線性回歸 (Linear Regression) 
邏輯回歸 (Logistic Regression)
最小化交叉熵：


二元分類（binary-class），顧名思義就是目標的類別僅有兩個。像是詐騙分析 (詐騙用戶 vs. 正常用戶)、瑕疵偵測 (瑕疵 vs. 正常)
多元分類（multi-class）則是目標類別有兩種以上。如手寫數字辨識有 10 個類別 (0~9)，影像競賽 ImageNet 更是有高達 1,000 個類別需要分類

多元分類（Multi-class）是每個樣本只能分在一個類別的任務
多標籤（Multi-label）是一個樣本可以同時存在有多個類別的任務


複習：https://www.youtube.com/watch?v=lNzcj_uwfak&t=2503s
## 線性回歸（Linear Regression）

- **目標**：預測連續數值（例如房價）。
- **模型形式**：對每筆資料 \(x_i\)，計算預測值  
  \[
    \hat y_i = w^\mathsf{T} x_i + b
  \]
- **參數學習**：找到最好的 \(w\) 和 \(b\)，讓下式「平方誤差之和」最小：  
  \[
    \min_{w,b}\;\sum_{i=1}^N \bigl(y_i - (w^\mathsf{T} x_i + b)\bigr)^2
  \]
  用文字描述就是：  
  > 對所有樣本 \(i\)，計算真實值 \(y_i\) 與預測值 \(w^\mathsf{T}x_i + b\) 的差，再把差值平方後相加，並調整 \(w,b\) 使這個總和最小。  
- **輸出**：任意實數（例如 \$312{,}500）。

---

## 邏輯回歸（Logistic Regression）

- **目標**：對二元標籤（0／1）做分類，輸出一個落在 \([0,1]\) 的機率。
- **模型形式**：  
  1. 先計算「線性分數」  
     \[
       z_i = w^\mathsf{T} x_i + b
     \]  
  2. 再套用 **sigmoid** 函數把它轉成機率  
     \[
       \hat p_i = \sigma(z_i) = \frac{1}{1 + e^{-z_i}}
     \]
- **參數學習**：找到最好的 \(w\) 和 \(b\)，讓下式「交叉熵損失」最小：  
  \[
    \min_{w,b}\; -\sum_{i=1}^N \Bigl[y_i \ln \hat p_i + (1 - y_i)\ln (1 - \hat p_i)\Bigr]
  \]
  用文字描述就是：  
  > 對所有樣本 \(i\)，把模型算出的機率 \(\hat p_i\) 與真實標籤 \(y_i\) 之間的「交叉熵」加總，並調整 \(w,b\) 使這個總和最小。  
- **輸出**：一個機率值，如果你設定門檻（例如 0.5），就可以把大於 0.5 的歸為 1，否則為 0。

---
